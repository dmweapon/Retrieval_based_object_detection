# pip install torch torchvision tqdm pillow qdrant-client
# pip install transformers==4.40.1
# pip install git+https://github.com/openai/CLIP.git@main

import os
import sys
import uuid
from pathlib import Path
from PIL import Image
from tqdm import tqdm

import torch
import clip
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

# -------------------------- ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ --------------------------
def load_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("ğŸ“¦ CLIP ViT-B/32 ëª¨ë¸ ë¡œë”© ì¤‘...")
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

# -------------------------- ì´ë¯¸ì§€ ì„ë² ë”© í•¨ìˆ˜ --------------------------
def embed_image_with_clip(image_path, model, preprocess, device):
    try:
        image = Image.open(image_path).convert("RGB")
        image_input = preprocess(image).unsqueeze(0).to(device)
        with torch.no_grad():
            embedding = model.encode_image(image_input).squeeze().cpu().numpy().tolist()
        return embedding
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì„ë² ë”© ì‹¤íŒ¨: {image_path} / {e}")
        return None

# -------------------------- ë©”ì¸ ì²˜ë¦¬ --------------------------
def main():
    # 1. ë°ì´í„°ì…‹ ê²½ë¡œ
    root_dir = input("dataset ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
    if not root_dir or not Path(root_dir).exists():
        print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤.")
        sys.exit(1)

    # 2. ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ
    img_type = input("ì‚¬ìš©í•  ì´ë¯¸ì§€ íƒ€ì… ì…ë ¥ (original / natural) [ê¸°ë³¸ê°’: original]: ").strip().lower()
    img_type = img_type if img_type in ["original", "natural"] else "original"
    base_dir = Path(root_dir) / f"{img_type}_images"
    if not base_dir.exists():
        print(f"âŒ {base_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # 3. í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ ì„ íƒ
    working_dir = {}
    all_classes = [d.name for d in base_dir.iterdir() if d.is_dir()]
    class_all = input("ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ì„ë² ë”©í• ê¹Œìš”? (y/n): ").strip().lower()
    if class_all == "y":
        for cls in all_classes:
            working_dir[cls] = base_dir / cls
    else:
        class_name = input("í´ë˜ìŠ¤ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
        if not (base_dir / class_name).exists():
            print("âŒ í•´ë‹¹ í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            sys.exit(1)
        working_dir[class_name] = base_dir / class_name

    # 4. Qdrant host ë° port ì…ë ¥
    print("[Q1] Qdrant Host ë° Port ì…ë ¥")
    host_input = input("í˜¸ìŠ¤íŠ¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš” [ê¸°ë³¸ê°’: localhost]: ").strip()
    host = host_input if host_input else "localhost"

    port_input = input("í¬íŠ¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 6333) [ê¸°ë³¸ê°’: 6333]: ").strip()
    try:
        port = int(port_input) if port_input else 6333
    except ValueError:
        print("âŒ ì˜ëª»ëœ í¬íŠ¸ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        sys.exit(1)

    # Qdrant ì—°ê²°
    try:
        client = QdrantClient(host=host, port=port)
        collections = client.get_collections().collections
        print("âœ… Vector Database(Qdrant)ì— ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 5. Qdrant collection ì„ íƒ
    if not collections:
        print("âŒ ìƒì„±ëœ collectionì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € collectionì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
        sys.exit(1)

    print("âœ… í˜„ì¬ collection ëª©ë¡:")
    for idx, col in enumerate(collections):
        print(f"{idx + 1}) {col.name}")
    col_idx = input("ì‚¬ìš©í•  collection ë²ˆí˜¸ ì…ë ¥: ").strip()
    try:
        collection_name = collections[int(col_idx) - 1].name
    except:
        print("âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤.")
        sys.exit(1)

    # 6. CLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model_dir = Path("model")
    model_dir.mkdir(exist_ok=True)
    model, preprocess, device = load_clip_model()

    # 7. ì´ë¯¸ì§€ ì„ë² ë”© ë° Qdrant ì €ì¥
    result_summary = {}
    for cls_name, cls_path in working_dir.items():
        image_files = [f for f in cls_path.iterdir() if f.suffix.lower() in [".png", ".jpg", ".jpeg"]]
        count = 0
        print(f"\nğŸ“¦ í´ë˜ìŠ¤ '{cls_name}' ì²˜ë¦¬ ì¤‘... ({len(image_files)}ì¥)")
        for img_path in tqdm(image_files, desc=f"  â†’ ì„ë² ë”© ì¤‘"):
            vector = embed_image_with_clip(img_path, model, preprocess, device)
            if vector is None:
                continue
            payload = {
                "class_name": cls_name,
                "image_path": str(img_path),
                "is_delegate": False,
                "delegate_type": "average"
            }
            point = PointStruct(id=str(uuid.uuid4()), vector=vector, payload=payload)
            client.upsert(collection_name=collection_name, points=[point])
            count += 1
        result_summary[cls_name] = count

    # 8. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ë³„ ì„ë² ë”© ìˆ˜:")
    for cls, cnt in result_summary.items():
        print(f"  - {cls}: {cnt}ê°œ")

    print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")
    sys.exit(0)

if __name__ == "__main__":
    main()