# pip install numpy qdrant-client scikit-learn tqdm

import os
import csv
import sys
import uuid
import datetime
import numpy as np
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
from sklearn.metrics import accuracy_score

# -------------------- í•˜ì´í¼íŒŒë¼ë¯¸í„° --------------------

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° - í…ŒìŠ¤íŠ¸ ê·¸ë£¹ ì„ íƒ
print("[Q0] ì‹¤í—˜í•  í…ŒìŠ¤íŠ¸ ê·¸ë£¹ì„ ì„ íƒí•˜ì„¸ìš”:")
testgroup_options = {
    "1": ("TestGroup1", "dataset_segmented"),
    "2": ("TestGroup2", "dataset_cropped")
}
for k, v in testgroup_options.items():
    print(f"{k}) {v[0]} ({v[1]})")
while True:
    selected = input("â†’ ë²ˆí˜¸ ì…ë ¥: ").strip()
    if selected in testgroup_options:
        test_group, root_dir = testgroup_options[selected]
        break
    print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

original_image_dir = "original_images"
natural_image_dir = "natural_images"

# Qdrant ì„¤ì •
qdrant_host = input("Qdrant í˜¸ìŠ¤íŠ¸ [ê¸°ë³¸ê°’: localhost]: ").strip() or "localhost"
qdrant_port = input("Qdrant í¬íŠ¸ [ê¸°ë³¸ê°’: 6333]: ").strip()
qdrant_port = int(qdrant_port) if qdrant_port else 6333

# Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = QdrantClient(host=qdrant_host, port=qdrant_port)
collections = [c.name for c in client.get_collections().collections]
if not collections:
    print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

print("\n[Q1] ì‚¬ìš©í•  Qdrant ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
for idx, name in enumerate(collections):
    print(f"{idx+1}) {name}")
while True:
    try:
        col_idx = int(input("â†’ ì»¬ë ‰ì…˜ ë²ˆí˜¸ ì…ë ¥: ")) - 1
        collection_name = collections[col_idx]
        break
    except:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ì‹¤í—˜ ì„¤ì •
cases = ['pre_a', 'pre_b', 'pre_c']
delegate_types = ['average', 'centroid', 'weighted', 'medoid']

# -------------------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ --------------------
def get_output_csv_path():
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    count = 1
    result_dir = Path("results")
    while True:
        subdir = result_dir / f"{today}-{count}"
        file_path = subdir / f"result_{today}-{count}.csv"
        if not file_path.exists():
            subdir.mkdir(parents=True, exist_ok=True)
            return file_path
        count += 1

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# -------------------- ì‹¤í—˜ ìˆ˜í–‰ --------------------

all_results = []
class_image_count = defaultdict(int)
all_scores = defaultdict(list)  # ì ìˆ˜ ì €ì¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬

print("\nì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ 12ê°œì˜ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")

for case in cases:
    # natural ì´ë¯¸ì§€ ë£¨í”„
    for class_name in sorted(os.listdir(f"{root_dir}/{natural_image_dir}")):
        class_dir = Path(root_dir) / natural_image_dir / class_name
        if not class_dir.is_dir():
            continue

        for img_file in tqdm(list(class_dir.glob("*.png")), desc=f"{case}/{class_name}"):
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë²¡í„° ì¡°íšŒ
            scroll_result, _ = client.scroll(
                collection_name=collection_name,
                scroll_filter=Filter(
                    must=[
                        FieldCondition(key="img_path", match=MatchValue(value=str(img_file))),
                        FieldCondition(key="is_delegate", match=MatchValue(value=False))
                    ]
                ),
                with_vectors=True,
                with_payload=True
            )
            if not scroll_result:
                continue

            test_vec = np.array(scroll_result[0].vector)
            test_payload = scroll_result[0].payload
            class_image_count[class_name] += 1

            # ê° ëŒ€í‘œ ë²¡í„° íƒ€ì…ë³„ ìœ ì‚¬ë„ ë¹„êµ
            for dtype in delegate_types:
                # caseì— ë”°ë¥¸ í•„í„° ì¡°ê±´ ë™ì  ìƒì„±
                filter_must = [
                    FieldCondition(key="delegate_type", match=MatchValue(value=dtype)),
                    FieldCondition(key="is_delegate", match=MatchValue(value=True)),
                    FieldCondition(key="class_name", match=MatchValue(value=class_name)),
                    FieldCondition(key="data_type", match=MatchValue(value=test_payload.get("data_type"))),
                ]
                if case == 'pre_a': # is_cropped=True, is_segmented=False, is_augmented=False
                    filter_must.extend([
                        FieldCondition(key="is_cropped", match=MatchValue(value=True)),
                        FieldCondition(key="is_segmented", match=MatchValue(value=False)),
                        FieldCondition(key="is_augmented", match=MatchValue(value=False)),
                    ])
                elif case == 'pre_b': # is_segmented=True
                     filter_must.extend([
                        FieldCondition(key="is_segmented", match=MatchValue(value=True)),
                        FieldCondition(key="is_augmented", match=MatchValue(value=False)),
                    ])
                elif case == 'pre_c': # is_augmented=True
                     filter_must.append(
                         FieldCondition(key="is_augmented", match=MatchValue(value=True))
                     )

                scroll_delegate, _ = client.scroll(
                    collection_name=collection_name,
                    scroll_filter=Filter(must=filter_must),
                    with_vectors=True,
                    with_payload=True,
                    limit=1
                )

                if not scroll_delegate:
                    continue

                ref_vec = np.array(scroll_delegate[0].vector)
                best_score = cosine_similarity(test_vec, ref_vec)
                best_class = scroll_delegate[0].payload.get("class_name")

                all_results.append({
                    "experiment_id": f"{case}_{dtype}",
                    "case": case,
                    "delegate_type": dtype,
                    "image_path": str(img_file),
                    "true_class": class_name,
                    "predicted_class": best_class,
                    "similarity_score": best_score
                })

                # ì ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ì— ì„ì‹œ ì €ì¥
                all_scores[f"{case}_{dtype}"].append(best_score)

# -------------------- CSV ë° NPY ì €ì¥ --------------------

output_path = get_output_csv_path()

# CSV íŒŒì¼ ì €ì¥
with open(output_path, 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=[
        "experiment_id", "case", "delegate_type", "image_path",
        "true_class", "predicted_class", "similarity_score"])
    writer.writeheader()
    writer.writerows(all_results)

print(f"\nâœ… ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

# NPY íŒŒì¼ ì €ì¥ ë° ìš”ì•½ ì¶œë ¥
print("\nğŸ—‚ï¸ NPY íŒŒì¼ ì €ì¥ ë° ìš”ì•½:")
score_dir = output_path.parent / "score_distribution"
score_dir.mkdir(parents=True, exist_ok=True)

for key, scores_list in sorted(all_scores.items()):
    score_path = score_dir / f"{key}_scores.npy"
    scores_np = np.array(scores_list)
    np.save(score_path, scores_np)

    print(f"\n- íŒŒì¼: {score_path}")
    if len(scores_np) > 0:
        print(f"  ì €ì¥ëœ ì ìˆ˜ ê°œìˆ˜: {len(scores_np)}")
        print(f"  ì ìˆ˜ ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 5ê°œ): {scores_np[:5]}")
        print(f"  í‰ê·  ì ìˆ˜: {np.mean(scores_np):.4f}")
    else:
        print(f"  ì €ì¥ëœ ì ìˆ˜ ì—†ìŒ")

# -------------------- ìš”ì•½ í†µê³„ ì¶œë ¥ --------------------

print("\nğŸ“Š natural ì´ë¯¸ì§€ ì‚¬ìš© í†µê³„:")
print("í´ë˜ìŠ¤ ìˆ˜:", len(class_image_count))
for cname, count in sorted(class_image_count.items()):
    print(f" - {cname}: {count}ì¥")