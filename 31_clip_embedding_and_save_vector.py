# pip install torch torchvision tqdm pillow qdrant-client
# pip install transformers==4.40.1
# pip install git+https://github.com/openai/CLIP.git@main

import os
import sys
import time
from pathlib import Path
import clip
import torch
from PIL import Image
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct

# Qdrant ì—°ê²° ì„¤ì • í•¨ìˆ˜
def connect_qdrant():
    print("[Qdrant] í˜¸ìŠ¤íŠ¸ì™€ í¬íŠ¸ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    host = input("Qdrant host [ê¸°ë³¸ê°’: localhost]: ").strip() or "localhost"
    while True:
        port_input = input("Qdrant port [ê¸°ë³¸ê°’: 6333]: ").strip()
        try:
            port = int(port_input) if port_input else 6333
            client = QdrantClient(host=host, port=port)
            client.get_collections()
            print("âœ… Vector Database(Qdrant)ì— ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return client
        except Exception as e:
            print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}\në‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")

# ì‚¬ìš©ì ì…ë ¥ ìœ í‹¸
def select_with_number(prompt, options, allow_back=False):
    while True:
        print(prompt)
        for i, opt in enumerate(options, 1):
            print(f"{i}) {opt}")
        if allow_back:
            print("b) ì²˜ìŒ ì§ˆë¬¸ìœ¼ë¡œ")
        choice = input("ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip().lower()
        if allow_back and choice == 'b':
            return 'back'
        if choice.isdigit() and 1 <= int(choice) <= len(options):
            return options[int(choice) - 1]
        print("âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_clip_model():
    model_path = Path("model") / "ViT-B-32.pt"
    model_path.parent.mkdir(exist_ok=True)
    if model_path.exists():
        print("âœ… ì´ë¯¸ ì €ì¥ë˜ì–´ìˆëŠ” CLIP ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ“¦ í•´ë‹¹ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ìˆì§€ ì•Šì•„ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
    return model, preprocess

# ì´ë¯¸ì§€ ì„ë² ë”© í•¨ìˆ˜
def embed_images_in_dir(model, preprocess, class_dir, class_name):
    image_files = list(Path(class_dir).glob("*.png"))
    embeddings = []
    for i, img_path in enumerate(image_files, 1):
        try:
            image = preprocess(Image.open(img_path)).unsqueeze(0)
            with torch.no_grad():
                embedding = model.encode_image(image)
                embedding /= embedding.norm(dim=-1, keepdim=True)
                embeddings.append((img_path.name, embedding.squeeze().tolist()))
            print(f"[{i}/{len(image_files)}] âœ… ì„ë² ë”© ì™„ë£Œ: {img_path.name}")
        except Exception as e:
            print(f"[{i}/{len(image_files)}] âŒ ì˜¤ë¥˜: {img_path.name} ({e})")
    return embeddings

# Qdrant ì €ì¥ í•¨ìˆ˜
def save_embeddings_to_qdrant(client, collection_name, class_name, embeddings):
    points = [
        PointStruct(
            id=i,
            vector=vec,
            payload={
                "class_name": class_name,
                "is_delegate": False,
                "delegate_type": "average"
            },
        ) for i, (name, vec) in enumerate(embeddings)
    ]
    client.upsert(collection_name=collection_name, points=points)
    print(f"âœ… {len(points)}ê°œì˜ ë²¡í„°ë¥¼ Qdrantì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ ë£¨í”„
def main():
    client = connect_qdrant()

    while True:
        # [ìš”ì²­1] dataset ì„ íƒ
        dataset_dir = select_with_number(
            "[1ë‹¨ê³„] ì‚¬ìš©í•  datasetì„ ì„ íƒí•´ì£¼ì„¸ìš”:",
            ["dataset_cropped", "dataset_segmented", "dataset_augmented"]
        )
        if dataset_dir == 'back':
            continue
        root_dir = Path(dataset_dir)

        # [ìš”ì²­2] ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ
        while True:
            img_type = select_with_number(
                "[2ë‹¨ê³„] ì‚¬ìš©í•  ì´ë¯¸ì§€ íƒ€ì…ì„ ì„ íƒí•´ì£¼ì„¸ìš”:",
                ["original", "natural"],
                allow_back=True
            )
            if img_type == 'back':
                break
            base_dir = root_dir / f"{img_type}_images"
            class_dirs = [d for d in base_dir.iterdir() if d.is_dir()]
            if not class_dirs:
                print("âŒ ì¤€ë¹„ëœ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (í•˜ìœ„ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤)\n")
                continue
            else:
                break
        if img_type == 'back':
            continue

        # [Q3] ì „ì²´ í´ë˜ìŠ¤ ì—¬ë¶€
        all_classes = input("[3ë‹¨ê³„] ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ì„ë² ë”©í• ê¹Œìš”? (y/n): ").strip().lower()
        working_dirs = {}

        if all_classes == 'y':
            for d in class_dirs:
                working_dirs[d.name] = d
        else:
            selected_class = select_with_number(
                "[3ë‹¨ê³„] í´ë˜ìŠ¤ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:",
                [d.name for d in class_dirs],
                allow_back=True
            )
            if selected_class == 'back':
                continue
            working_dirs[selected_class] = base_dir / selected_class

        # [Q4] collection ì„ íƒ
        collections = client.get_collections().collections
        collection_names = [c.name for c in collections]
        collection_name = select_with_number(
            "[4ë‹¨ê³„] ì €ì¥í•  Collectionì„ ì„ íƒí•´ì£¼ì„¸ìš”:",
            collection_names,
            allow_back=True
        )
        if collection_name == 'back':
            continue

        # [Q5] ëª¨ë¸ ì„ íƒ
        print("[5ë‹¨ê³„] ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”:")
        print("1) CLIP ViT-B/32")
        model_choice = input("ëª¨ë¸ ë²ˆí˜¸ ì…ë ¥ [ê¸°ë³¸ê°’: 1]: ").strip()
        model_choice = model_choice if model_choice else "1"
        if model_choice != "1":
            print("âŒ í˜„ì¬ëŠ” CLIP ViT-B/32ë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
            continue

        model, preprocess = load_clip_model()

        # [Q6] ì„ë² ë”© ë° ì €ì¥
        total_counts = {}
        for class_name, class_path in working_dirs.items():
            print(f"\nğŸš€ {class_name} í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            embeddings = embed_images_in_dir(model, preprocess, class_path, class_name)
            save_embeddings_to_qdrant(client, collection_name, class_name, embeddings)
            total_counts[class_name] = len(embeddings)

        # [Q7] ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š í´ë˜ìŠ¤ë³„ ì„ë² ë”© ìˆ˜:")
        for k, v in total_counts.items():
            print(f" - {k}: {v}ê°œ")

        # [Q8] ì¶”ê°€ ì‘ì—… ì—¬ë¶€
        again = input("\në‹¤ë¥¸ í´ë˜ìŠ¤ë„ ì´ì–´ì„œ ì‘ì—…í• ê¹Œìš”? (y/n): ").strip().lower()
        if again != 'y':
            print("ğŸ‘‹ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

if __name__ == "__main__":
    main()