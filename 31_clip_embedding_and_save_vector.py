import os
import sys
import clip
import torch
import qdrant_client
from PIL import Image
from pathlib import Path
from qdrant_client.http import models
from qdrant_client.models import PointStruct
from tqdm import tqdm


def get_valid_input(prompt, options):
    while True:
        print(prompt)
        for key, val in options.items():
            print(f"{key}) {val}")
        choice = input("ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip().lower()
        if choice in options:
            return options[choice]
        elif choice == 'b':
            return 'back'
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


def connect_qdrant():
    print("[Qdrant ì—°ê²° ì„¤ì •]")
    host = input("Qdrant í˜¸ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” [ê¸°ë³¸ê°’: localhost]: ").strip() or "localhost"
    port_input = input("Qdrant í¬íŠ¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 6333) [ê¸°ë³¸ê°’: 6333]: ").strip()
    port = 6333 if port_input == "" else int(port_input)
    try:
        client = qdrant_client.QdrantClient(host=host, port=port)
        client.get_collections()
        print("âœ… Vector Database(Qdrant)ì— ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return client
    except Exception as e:
        print("âŒ Qdrant ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:", e)
        sys.exit(1)


def list_subdirectories(path):
    return [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]


def select_directory(prompt, directories):
    while True:
        print(prompt)
        for i, d in enumerate(directories, 1):
            print(f"{i}) {d}")
        print("b) ì´ì „ ì§ˆë¬¸ìœ¼ë¡œ")
        choice = input("ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip().lower()
        if choice == 'b':
            return 'back'
        if choice.isdigit() and 1 <= int(choice) <= len(directories):
            return directories[int(choice) - 1]
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


def get_embedding_model(model_dir):
    model_options = {'1': 'clip-ViT-B/32'}
    model_choice = get_valid_input("[Q6] ì‚¬ìš©í•  Embedding ëª¨ë¸ ì„ íƒ", model_options)
    model_name = model_choice if model_choice != 'back' else 'back'

    if model_name == 'clip-ViT-B/32':
        model_path = os.path.join(model_dir, model_name)
        if os.path.exists(model_path):
            print("âœ… ì´ë¯¸ ì €ì¥ë˜ì–´ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print("â¬‡ï¸ í•´ë‹¹ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ìˆì§€ ì•Šì•„ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        model, preprocess = clip.load("ViT-B/32", device="cuda" if torch.cuda.is_available() else "cpu")
        return model_name, model, preprocess
    return None, None, None


def save_embeddings_to_qdrant(client, collection_name, vectors, class_name, img_paths):
    points = []
    for i, vector in enumerate(vectors):
        point = PointStruct(
            id=i,
            vector=vector,
            payload={
                "class_name": class_name,
                "is_delegate": False,
                "delegate_type": "average",
                "img_path": img_paths[i]
            }
        )
        points.append(point)
    client.upsert(collection_name=collection_name, points=points)


def main():
    model_dir = "model"
    client = connect_qdrant()

    while True:
        # [Q1] dataset ê²½ë¡œ ì…ë ¥
        dataset_options = {'1': 'dataset_cropped', '2': 'dataset_segmented', '3': 'dataset_augmented'}
        root_dir = get_valid_input("[Q1] ì‚¬ìš©í•  Dataset ê²½ë¡œ ì„ íƒ", dataset_options)
        if root_dir == 'back':
            continue

        # [Q2] ì´ë¯¸ì§€ íƒ€ì… ì…ë ¥
        while True:
            img_type_options = {'1': 'original', '2': 'natural'}
            img_type = get_valid_input("[Q2] ì‚¬ìš©í•  ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ", img_type_options)
            if img_type == 'back':
                break
            base_dir = os.path.join(root_dir, f"{img_type}_images")
            if not os.path.exists(base_dir) or not list_subdirectories(base_dir):
                print("âŒ ì¤€ë¹„ëœ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (í•˜ìœ„ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤)")
                continue
            else:
                break

        if img_type == 'back':
            continue

        # [Q3] í´ë˜ìŠ¤ ì„ íƒ
        all_classes = list_subdirectories(base_dir)
        all_choice = input("ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ì„ë² ë”©í• ê¹Œìš”? (y/n): ").strip().lower()
        if all_choice == 'y':
            working_dirs = [os.path.join(base_dir, cls) for cls in all_classes]
        else:
            class_name = select_directory("[Q3] ì„ë² ë”©í•  í´ë˜ìŠ¤ ì„ íƒ", all_classes)
            if class_name == 'back':
                continue
            working_dirs = [os.path.join(base_dir, class_name)]

        # [Q4] collection ì„ íƒ
        collections = client.get_collections().collections
        collection_names = [col.name for col in collections]
        collection_map = {str(i + 1): name for i, name in enumerate(collection_names)}
        print("[Q4] í˜„ì¬ Qdrantì— ì €ì¥ëœ Collections")
        for key, val in collection_map.items():
            print(f"{key}) {val}")
        selected_idx = input("ì‚¬ìš©í•  Collection ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        collection_name = collection_map.get(selected_idx)
        if not collection_name:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # [Q5] ëª¨ë¸ ì„ íƒ
        model_name, model, preprocess = get_embedding_model(model_dir)
        if model_name == 'back':
            continue

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.eval()

        # [Embedding ìˆ˜í–‰]
        total_summary = {}
        for dir_path in working_dirs:
            class_name = Path(dir_path).name
            image_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.lower().endswith(".png")]
            embeddings = []
            img_paths = []
            for img_path in tqdm(image_files, desc=f"[ì„ë² ë”©] {class_name}"):
                try:
                    image = preprocess(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
                    with torch.no_grad():
                        embedding = model.encode_image(image).squeeze().cpu().tolist()
                    embeddings.append(embedding)
                    img_paths.append(str(Path(img_path).relative_to(Path.cwd())))
                except Exception as e:
                    print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {img_path}, ì—ëŸ¬: {e}")

            save_embeddings_to_qdrant(client, collection_name, embeddings, class_name, img_paths)
            total_summary[class_name] = len(embeddings)

        print("\n[ì‘ì—… ìš”ì•½]")
        for cls, count in total_summary.items():
            print(f"í´ë˜ìŠ¤: {cls} - ì„ë² ë”© ìˆ˜: {count}")

        again = input("\nğŸ“Œ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ë„ ê³„ì† ì‘ì—…í• ê¹Œìš”? (y/n): ").strip().lower()
        if again != 'y':
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


if __name__ == "__main__":
    main()