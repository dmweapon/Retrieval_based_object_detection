# pip install torch torchvision tqdm pillow qdrant-client
# pip install transformers==4.40.1
# pip install git+https://github.com/openai/CLIP.git@main

# pip install torch torchvision tqdm pillow qdrant-client
# pip install transformers==4.40.1
# pip install git+https://github.com/openai/CLIP.git@main

import os
import sys
import uuid
import hashlib
from pathlib import Path
from PIL import Image
from tqdm import tqdm

import torch
import clip
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

# -------------------------- ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ --------------------------
def load_clip_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("ğŸ“¦ CLIP ViT-B/32 ëª¨ë¸ ë¡œë”© ì¤‘...")
    model, preprocess = clip.load("ViT-B/32", device=device)
    return model, preprocess, device

# -------------------------- ì´ë¯¸ì§€ ì„ë² ë”© í•¨ìˆ˜ --------------------------
def embed_image_with_clip(image_path, model, preprocess, device):
    try:
        image = Image.open(image_path).convert("RGB")
        image_input = preprocess(image).unsqueeze(0).to(device)
        with torch.no_grad():
            embedding = model.encode_image(image_input).squeeze().cpu().numpy().tolist()
        return embedding
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì„ë² ë”© ì‹¤íŒ¨: {image_path} / {e}")
        return None

# -------------------------- ê³ ìœ  ID ìƒì„± í•¨ìˆ˜ --------------------------
def generate_id_from_path(img_path: Path) -> str:
    return hashlib.md5(str(img_path.resolve()).encode()).hexdigest()

# -------------------------- ë©”ì¸ ì²˜ë¦¬ --------------------------
def main():
    print("[Q0] Qdrant Host ë° Port ì…ë ¥")
    host_input = input("Qdrant í˜¸ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” [ê¸°ë³¸ê°’: localhost]: ").strip()
    host = host_input if host_input else "localhost"

    while True:
        port_input = input("Qdrant í¬íŠ¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 6333) [ê¸°ë³¸ê°’: 6333]: ").strip()
        try:
            port = int(port_input) if port_input else 6333
            client = QdrantClient(host=host, port=port)
            collections = client.get_collections().collections
            print("âœ… Qdrantì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
            print("ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 1. CLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model_dir = Path("model")
    model_dir.mkdir(exist_ok=True)
    model, preprocess, device = load_clip_model()

    while True:
        # 2. dataset ê²½ë¡œ ì„ íƒ
        dataset_options = {
            "1": "dataset_cropped",
            "2": "dataset_segmented",
            "3": "dataset_augmented"
        }
        while True:
            print("[Q1] ì‚¬ìš©í•  Dataset ë””ë ‰í† ë¦¬ ì„ íƒ")
            for k, v in dataset_options.items():
                print(f"{k}) {v}")
            selected = input("ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if selected in dataset_options:
                root_dir = dataset_options[selected]
                break
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # 3. ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ
        img_type_options = {"1": "original", "2": "natural"}
        while True:
            print("[Q2] ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ (original / natural)")
            for k, v in img_type_options.items():
                print(f"{k}) {v}")
            img_type_input = input("ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if img_type_input in img_type_options:
                img_type = img_type_options[img_type_input]
                break
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # 4. í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ ì„ íƒ
        while True:
            base_dir = Path(root_dir) / f"{img_type}_images"
            if not base_dir.exists():
                print(f"âŒ {base_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                sys.exit(1)

            class_dirs = [d for d in base_dir.iterdir() if d.is_dir()]
            if not class_dirs:
                print("âŒ ì¤€ë¹„ëœ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            working_dir = {}
            all_classes = [d.name for d in class_dirs]
            while True:
                class_all = input("ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ì„ë² ë”©í• ê¹Œìš”? (y/n): ").strip().lower()
                if class_all == "y":
                    for cls in all_classes:
                        working_dir[cls] = base_dir / cls
                    break
                elif class_all == "n":
                    print("[Q3] í´ë˜ìŠ¤ ëª©ë¡")
                    for idx, name in enumerate(all_classes):
                        print(f"{idx+1}) {name}")
                    while True:
                        selected = input("í´ë˜ìŠ¤ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
                        try:
                            idx = int(selected) - 1
                            class_name = all_classes[idx]
                            class_path = base_dir / class_name
                            working_dir[class_name] = class_path
                            break
                        except:
                            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    break
                else:
                    print("âŒ y ë˜ëŠ” në§Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            break

        # 5. collection ì„ íƒ
        while True:
            if not collections:
                print("âŒ ìƒì„±ëœ collectionì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € collectionì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
                sys.exit(1)
            print("âœ… í˜„ì¬ collection ëª©ë¡:")
            for idx, col in enumerate(collections):
                print(f"{idx + 1}) {col.name}")
            col_idx = input("ì‚¬ìš©í•  collection ë²ˆí˜¸ ì…ë ¥: ").strip()
            try:
                collection_name = collections[int(col_idx) - 1].name
                break
            except:
                print("âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ë²ˆí˜¸ë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # 6. ì´ë¯¸ì§€ ì„ë² ë”© ë° Qdrant ì €ì¥
        result_summary = {}

        is_segmented = root_dir == "dataset_segmented"
        is_augmented = root_dir == "dataset_augmented"

        for cls_name, cls_path in working_dir.items():
            image_files = [f for f in cls_path.iterdir() if f.suffix.lower() in [".png", ".jpg", ".jpeg"]]
            count = 0
            print(f"\nğŸ“¦ í´ë˜ìŠ¤ '{cls_name}' ì²˜ë¦¬ ì¤‘... ({len(image_files)}ì¥)")
            for img_path in tqdm(image_files, desc=f"  â†’ ì„ë² ë”© ì¤‘"):
                vector = embed_image_with_clip(img_path, model, preprocess, device)
                if vector is None:
                    continue

                payload = {
                    "data_type": f"{img_type}_images",     # original_images ë˜ëŠ” natural_images
                    "is_cropped": True,
                    "is_segmented": is_segmented,
                    "is_augmented": is_augmented,
                    "class_name": cls_name,
                    "is_delegate": False,
                    "delegate_type": None,
                    "img_path": str(img_path)
                }

                point_id = generate_id_from_path(img_path)  # ì´ë¯¸ì§€ ê²½ë¡œ ê¸°ë°˜ ê³ ìœ  ID
                point = PointStruct(id=point_id, vector=vector, payload=payload)
                client.upsert(collection_name=collection_name, points=[point])
                count += 1
            result_summary[cls_name] = count

        # 7. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ë³„ ì„ë² ë”© ìˆ˜:")
        for cls, cnt in result_summary.items():
            print(f"  - {cls}: {cnt}ê°œ")

        # 8. ë‹¤ìŒ ì‘ì—… ì—¬ë¶€ í™•ì¸
        cont = input("\nâ• ë‹¤ë¥¸ ì‘ì—…ì„ ì´ì–´ì„œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if cont != 'y':
            print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")
            sys.exit(0)

if __name__ == "__main__":
    main()