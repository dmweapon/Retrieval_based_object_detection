# pip install torch torchvision tqdm pillow qdrant-client
# pip install transformers==4.40.1
# pip install git+https://github.com/openai/CLIP.git@main

import os
from pathlib import Path
import torch
import clip
from PIL import Image
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct, CollectionStatus

# -----------------------------
# Qdrant í˜¸ìŠ¤íŠ¸ ë° í¬íŠ¸ ì„¤ì •
# -----------------------------
def get_qdrant_connection():
    while True:
        host = input("[Q1] Qdrant í˜¸ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” [ê¸°ë³¸ê°’: localhost]: ").strip() or "localhost"
        port_input = input("[Q2] Qdrant í¬íŠ¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” [ê¸°ë³¸ê°’: 6333]: ").strip()
        port = 6333 if port_input == "" else int(port_input)

        try:
            client = QdrantClient(host=host, port=port, timeout=5.0)
            client.get_collections()  # ì—°ê²° í…ŒìŠ¤íŠ¸
            print("\nâœ… Vector Database(Qdrant)ì— ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return client
        except Exception as e:
            print(f"\nâŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}\në‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# ìœ ì‚¬ë„ ê±°ë¦¬ ë°©ì‹ ì„ íƒ
# -----------------------------
def select_distance():
    distance_options = {
        "1": Distance.COSINE,
        "2": Distance.EUCLID,
        "3": Distance.DOT,
        "4": Distance.MANHATTAN
    }
    while True:
        print("\n[Q3] ìœ ì‚¬ë„ ê±°ë¦¬ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1) Cosine\n2) Euclid\n3) Dot\n4) Manhattan")
        choice = input("ì„ íƒ (1-4): ").strip()
        if choice in distance_options:
            return distance_options[choice]
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# Collection ì´ë¦„ ì…ë ¥
# -----------------------------
def input_collection_name():
    while True:
        name = input("\n[Q4] ì‚¬ìš©í•  Collection ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
        if name:
            return name
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# Collection ìƒì„± ë˜ëŠ” ì¡´ì¬ í™•ì¸
# -----------------------------
def create_collection_if_needed(client, name, distance):
    collections = client.get_collections().collections
    if any(c.name == name for c in collections):
        print(f"\nâœ… ì´ë¯¸ ì¡´ì¬í•˜ëŠ” Collectionì…ë‹ˆë‹¤: {name}")
    else:
        client.create_collection(
            collection_name=name,
            vectors_config=VectorParams(size=512, distance=distance)
        )
        print(f"\nâœ… Collection ìƒì„± ì™„ë£Œ: {name}")

# -----------------------------
# dataset ë””ë ‰í† ë¦¬ ì„ íƒ
# -----------------------------
def select_dataset_dir():
    options = {"1": "dataset_cropped", "2": "dataset_segmented", "3": "dataset_augmented"}
    while True:
        print("\n[Q5] Dataset ë””ë ‰í† ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        for k, v in options.items():
            print(f"{k}) {v}")
        choice = input("ì„ íƒ: ").strip()
        if choice in options:
            return options[choice]
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# ì´ë¯¸ì§€ íƒ€ì… ì„ íƒ
# -----------------------------
def select_img_type():
    options = {"1": "original", "2": "natural"}
    while True:
        print("\n[Q6] ì‚¬ìš©í•  ì´ë¯¸ì§€ íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”:")
        for k, v in options.items():
            print(f"{k}) {v}")
        choice = input("ì„ íƒ: ").strip()
        if choice in options:
            return options[choice]
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ ì„ íƒ
# -----------------------------
def select_class_name(base_dir):
    while True:
        subdirs = [d.name for d in Path(base_dir).iterdir() if d.is_dir()]
        if not subdirs:
            print("\nâŒ ì¤€ë¹„ëœ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (í•˜ìœ„ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤)")
            return None

        print("\n[Q7] í´ë˜ìŠ¤ ë””ë ‰í† ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        for i, name in enumerate(subdirs, 1):
            print(f"{i}) {name}")
        print("b) ì´ì „ ì§ˆë¬¸ìœ¼ë¡œ")

        choice = input("ì„ íƒ: ").strip()
        if choice == "b":
            return "back"
        if choice.isdigit() and 1 <= int(choice) <= len(subdirs):
            return subdirs[int(choice) - 1]
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

# -----------------------------
# ì´ë¯¸ì§€ ë¡œë“œ ë° ì„ë² ë”©
# -----------------------------
def embed_images(image_paths, model, preprocess):
    embeddings = []
    for path in image_paths:
        try:
            image = preprocess(Image.open(path).convert("RGB")).unsqueeze(0).to("cuda")
            with torch.no_grad():
                embedding = model.encode_image(image)
                embedding /= embedding.norm(dim=-1, keepdim=True)
                embeddings.append(embedding.squeeze().cpu().numpy())
        except Exception as e:
            print(f"âš ï¸  {path} ì„ë² ë”© ì‹¤íŒ¨: {e}")
    return embeddings

# -----------------------------
# ë²¡í„°ë¥¼ Qdrantì— ì €ì¥
# -----------------------------
def save_embeddings_to_qdrant(embeddings, img_paths, collection_name, class_name):
    points = []
    for i, (embedding, img_path) in enumerate(zip(embeddings, img_paths)):
        relative_path = str(img_path.relative_to(Path.cwd()))
        payload = {
            "class_name": class_name,
            "is_delegate": False,
            "delegate_type": "average",
            "img_path": relative_path
        }
        point = PointStruct(id=f"{class_name}_{i}", vector=embedding, payload=payload)
        points.append(point)

    qdrant_client.upsert(
        collection_name=collection_name,
        wait=True,
        points=points,
    )

# -----------------------------
# ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# -----------------------------
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)

    qdrant_client = get_qdrant_connection()
    distance_metric = select_distance()
    collection_name = input_collection_name()
    create_collection_if_needed(qdrant_client, collection_name, distance_metric)

    while True:
        dataset_dir = select_dataset_dir()
        img_type = select_img_type()

        base_dir = Path(dataset_dir) / img_type
        class_name = select_class_name(base_dir)
        if class_name is None:
            continue
        if class_name == "back":
            continue

        class_path = base_dir / class_name
        image_paths = list(class_path.glob("*.png")) + list(class_path.glob("*.jpg"))
        if not image_paths:
            print("\nâŒ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            continue

        print(f"\nğŸ” {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì„ë² ë”© ì¤‘...")
        vectors = embed_images(image_paths, model, preprocess)

        print("ğŸ’¾ ë²¡í„°ë¥¼ Qdrantì— ì €ì¥ ì¤‘...")
        save_embeddings_to_qdrant(vectors, image_paths, collection_name, class_name)
        print(f"âœ… ì™„ë£Œ: {class_name} í´ë˜ìŠ¤ì˜ {len(vectors)}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ\n")

        again = input("ê³„ì†í•´ì„œ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¥¼ ì‘ì—…í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if again != "y":
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
